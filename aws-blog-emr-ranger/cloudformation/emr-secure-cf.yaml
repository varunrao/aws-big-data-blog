Description: CloudFormation template for creating an EMR cluster
Outputs:
  IPAddress:
    Description: IP address of EMR cluster MasterNode
    Value: !GetAtt [rEMRCluster, MasterPublicDNS]
Parameters:
  pExperianLANid:
    Description: User's Experian LAN ID
    Type: String
  pCoreInstanceCount:
    Default: '1'
    Description: Number of core instances
    Type: Number
    MinValue: 1
    MaxValue: 25
    ConstraintDescription: Number of core nodes should be within 1-20
  pCoreInstanceType:
    Default: m3.xlarge
    AllowedValues:
      - m3.xlarge
      - m3.2xlarge
      - m4.2xlarge
      - m4.4xlarge
      - r4.2xlarge
      - r4.4xlarge
      - r4.8xlarge
    ConstraintDescription: Instance types should be one of the following m3.xlarge, m3.2xlarge, m4.2xlarge, m4.4xlarge, r4.2xlarge, r4.4xlarge
    Description: Instance Type of the core node
    Type: String
  pCoreStorageVolume:
    Default: 10
    AllowedValues:
      - 10
      - 20
      - 30
      - 50
      - 100
      - 200
    ConstraintDescription: Attached storage to coe nodes is limited
    Description: Code node storage volume attachment in GB
    Type: String
  pUseSpot:
    Default: false
    Description: true/false - if we should use spot
    AllowedValues:
      - true
      - false
    Type: String
  pTerminateCluster:
    Default: true
    Description: Should the cluster be terminated after use?
    AllowedValues:
      - true
      - false
    Type: String
  pCoreBidPrice:
    Default: 0.30
    Description: Bid Price for Core instances if using Spot
    Type: Number
    MaxValue: 1.2
    ConstraintDescription: Bid price cannot exceed $1.2
  pEMRClusterName:
    Default: EMR-R8-Dev
    Description: Cluster name for the EMR
    Type: String
    AllowedValues:
      - EMR-R8-Dev
  pEMRLogDir:
    Default: emrlogs
    Description: Log Dir for the EMR cluster
    Type: String
    AllowedValues:
      - emrlogs
  pKeyName:
    Default: create-emr
    Description: Name of an existing EC2 KeyPair to enable SSH to the instances
    Type: AWS::EC2::KeyPair::KeyName
  pMasterInstanceType:
    Default: m3.xlarge
    Description: Instance Type of the master node
    Type: String
    AllowedValues:
      - m3.xlarge
      - m3.2xlarge
      - m4.2xlarge
      - m4.4xlarge
      - r4.2xlarge
      - r4.4xlarge
      - r4.8xlarge
#  pVPC:
#    Type: AWS::EC2::VPC::Id
#    Default: vpc-69a01110
#    Description: Select a VPC to deploy the Lambda function and EMR cluster
#  pSubnet:
#    Description: Subnet ID for creating the EMR cluster
#    Default: subnet-ea5f36a2
#    Type: AWS::EC2::Subnet::Id
  pEmrReleaseLabel:
    Default: emr-5.6.0
    Description: Release label for the EMR cluster
    Type: String
    AllowedValues:
      - emr-5.6.0
      - emr-5.0.0
      - emr-4.8.4
  pS3EMRBucketName:
    Default: r8-aws-emr
    Description: Bucket with the security configuration files
    Type: String
    AllowedValues:
      - r8-aws-emr
#  pS3CertsKey:
#    Default: automation/emr/certsnew.zip
#    Description: Bucket with the security configuration files
#    Type: String
#  pS3EMRBucket:
 #   Default: s3://r8-aws-emr
  #  Description: Bucket with the step files
   # Type: String
  pS3BAPath:
    Default: admin/bootstrap/download-emr-steps.sh
    AllowedValues:
      - admin/bootstrap/download-emr-steps.sh
    Description: location for BA script
    Type: String
  pS3StepsPath:
    Default: steps/emr-steps.json
    Description: location for steps json
    Type: String
    AllowedValues:
      - steps/emr-steps.json
#  pAdditionalSG:
#    Default: sg-93d802e2
#    Description: Additional SG to assign
#    Type: String
#  pSquidProxyPort:
#    Default: 3128
#    Description: Port for proxy
#    Type: String
#  pSquidProxyHost:
#    Default: internal-R8SquidPr-ElasticL-6N6FNTA68BK2-1003731070.us-east-1.elb.amazonaws.com
#    Description: host for proxy
#    Type: String
#  pDBPassword:
#    Default: r8EMRHive
#    Description: Hive metastore DB password
#    Type: String
#    NoEcho: true
  pEMRSSHSg:
    Default: sg-59a81f28
    Description: Additional SG to assign
    Type: String
    AllowedValues:
      - sg-59a81f28
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "User Configuration"
        Parameters:
          - pExperianLANid
      -
        Label:
          default: "EMR cluster configuration"
        Parameters:
          - pTerminateCluster
          - pCoreInstanceType
          - pEmrReleaseLabel
          - pUseSpot
          - pCoreBidPrice
          - pUseSpot
      -
        Label:
          default: "Defaults"
        Parameters:
          - pS3EMRBucketName
          - pEMRSSHSg
          - pEMRLogDir
          - pDBPassword
          - pEMRClusterName
          - pS3StepsPath
          - pS3BAPath
          - pMasterInstanceType
    ParameterLabels:
      pExperianLANid:
        default: "Please specify your Experian LAN ID"
      pTerminateCluster:
        default: "Set this to true if you want the cluster to terminate"
Mappings:
    Config:
        Hive:
            DevMetastorePassword: r8EMRHive
Conditions:
  UseSpot: !Equals [!Ref pUseSpot, true]
  DoNoTerminate: !Equals [!Ref pTerminateCluster, false]
  Terminate: !Equals [!Ref pTerminateCluster, true]
  OlderVersion: !Equals [!Ref pEmrReleaseLabel, 'emr-4.8.4']
Resources:
  pKMSLUKSKey:
    Properties:
      Description: Master Key that will be used for LUKS Encryption
      Enabled: 'true'
      KeyPolicy:
        Statement:
        - Action:
          - kms:Encrypt
          - kms:Decrypt
          - kms:GenerateDataKey
          Effect: Allow
          Principal:
            AWS:
            - !Join ['', ['arn:aws:iam::', !Ref 'AWS::AccountId', ':role/EMR_EC2_DefaultRole']]
          Resource:
          - '*'
          Sid: Stmt3
        - Action:
          - kms:Put*
          - kms:ScheduleKeyDeletion
          - kms:CancelKeyDeletion
          - kms:Describe*
          - kms:Revoke*
          - kms:Disable*
          - kms:Enable*
          - kms:Delete*
          - kms:List*
          - kms:Update*
          - kms:Create*
          Effect: Allow
          Principal:
            AWS:
            - !Join ['', ['arn:aws:iam::', !Ref 'AWS::AccountId', ':root']]
          Resource:
          - '*'
          Sid: Stmt4
        Version: '2012-10-17'
    Type: AWS::KMS::Key

  rLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Policies:
      - PolicyName: enicreateaccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:CreateNetworkInterface
            - ec2:DescribeNetworkInterfaces
            - ec2:DeleteNetworkInterface
            Resource: '*'
      - PolicyName: lambdalogtocloudwatch
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
#            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
      - PolicyName: s3listaccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:ListBucket
            Resource:
              - arn:aws:s3:::r8-aws-emr/*
              - arn:aws:s3:::r8-aws-devops/*
      - PolicyName: s3putaccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:GetObject
            - s3:PutObject
            - s3:DeleteObject
            Resource:
              - arn:aws:s3:::r8-aws-emr/*
              - arn:aws:s3:::r8-aws-devops/*
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaENIManagementAccess
  rLambdaSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Lambda security group
      VpcId: !ImportValue R8VPC
      SecurityGroupEgress:
      - CidrIp: 0.0.0.0/0
        FromPort: '-1'
        IpProtocol: '-1'
        ToPort: '-1'
  rCreateSecurityConfigurationFiles:
    Type: Custom::LambdaCallout
    DependsOn: rSecurityConfigurationFunction
    Properties:
      ServiceToken: !GetAtt [rSecurityConfigurationFunction, Arn]
#  rEMRLambdaLogGroup:
#   Type: "AWS::Logs::LogGroup"
#   Properties:
#    LogGroupName: !Join ['', ['/aws/lambda/', !Ref rSecurityConfigurationFunction]]
#    RetentionInDays: 14

  rSecurityConfigurationFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          from __future__ import print_function
          import json
          import zipfile
          import boto3
          import os
          import subprocess
          import commands
          import cfnresponse
          def lambda_handler(event, context):
              s3_client = boto3.client('s3')
              s3_bucket = boto3.resource('s3').Bucket(os.environ['bucketname'])
              responseData = {}
              if event['RequestType'] == 'Delete':
                responseData['Data'] = 'SUCCESS'
                s3_client.delete_object(Bucket=os.environ['bucketname'], Key="%s/%s" % (os.environ['pExperianLANid'],"certsnew.zip"))
                cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, "CustomLambdaCerts")
              else:
                s3_client.download_file(os.environ['bucketname'], 'admin/security/create-certs.sh', '/tmp/create-certs.sh')

                command_output = (commands.getstatusoutput('bash /tmp/create-certs.sh'))
                print("{}".format(command_output))
                if str(command_output).index("SUCCESS"):
                    responseData['Data'] = 'SUCCESS'
                    zf = zipfile.ZipFile("/tmp/certsnew.zip", "w")
                    for dirname, subdirs, files in os.walk("/tmp/certs"):
                      for filename in files:
                        if filename.endswith("pem"):
                          zf.write(os.path.join(dirname, filename), filename)
                    zf.close()
                    data = open('/tmp/certsnew.zip', 'rb')
                    s3_bucket.put_object(Body=data, ServerSideEncryption='AES256', Key="%s/%s" % (os.environ['pExperianLANid'],"certsnew.zip"))
                    cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, "CustomLambdaCerts")
                else:
                    responseData['Data'] = 'FAILED'
                    cfnresponse.send(event, context, cfnresponse.FAILED, responseData, "CustomLambdaCerts")
                return 'Completed with status {}'.format(responseData['Data'])
      Handler: index.lambda_handler
      Environment:
        Variables:
          bucketname: !Ref pS3EMRBucketName
          pExperianLANid: !Ref pExperianLANid
      Runtime: python2.7
#      VpcConfig:
#        SecurityGroupIds:
#          - !Ref rLambdaSecurityGroup
#        SubnetIds:
#          - !Ref pSubnet
      Timeout: '30'
      Role: !GetAtt [rLambdaExecutionRole, Arn]
  rSecurityConfiguration:
    DependsOn: rCreateSecurityConfigurationFiles
    Type: 'AWS::EMR::SecurityConfiguration'
    Properties:
      SecurityConfiguration:
        EncryptionConfiguration:
          EnableInTransitEncryption: true
          EnableAtRestEncryption: true
          InTransitEncryptionConfiguration:
            TLSCertificateConfiguration:
              CertificateProviderType: PEM
              S3Object: !Join ['', ['s3://', !Ref 'pS3EMRBucketName', '/', !Ref 'pExperianLANid', '/certsnew.zip']]
          AtRestEncryptionConfiguration:
            S3EncryptionConfiguration:
              EncryptionMode: SSE-S3
            LocalDiskEncryptionConfiguration:
              EncryptionKeyProviderType: AwsKms
              AwsKmsKey: !GetAtt [pKMSLUKSKey, Arn]
  rEMRCluster:
    Properties:
      AdditionalInfo: {'instanceAwsClientConfiguration' : {'proxyPort' : !ImportValue SquidProxyPort, 'proxyHost' : !ImportValue SquidProxyDNS} }
      SecurityConfiguration: !Ref rSecurityConfiguration
      Applications:
      - Name: Hive
      - Name: Spark
      - Name: Hue
      - Name: !If [OlderVersion, 'Oozie-Sandbox', 'Oozie']
      Configurations:
      - Classification: hive-site
        ConfigurationProperties:
          javax.jdo.option.ConnectionURL:
            Fn::Join:
            - ''
            - - jdbc:mysql://
              - !ImportValue HiveMetastoreHost
              - ":"
              - !ImportValue HiveMetastorePort
              - "/"
              - hive?createDatabaseIfNotExist=true
          javax.jdo.option.ConnectionUserName: !ImportValue HiveMetastoreDBUsername
          javax.jdo.option.ConnectionPassword: !FindInMap [ Config, Hive, DevMetastorePassword ]
        Configurations: []
      - Classification: hadoop-env
        Configurations:
          - Classification: "export"
            Configurations: []
            ConfigurationProperties:
              JAVA_HOME: /usr/lib/jvm/java-1.8.0
      - Classification: spark-env
        Configurations:
          - Classification: "export"
            Configurations: []
            ConfigurationProperties:
              JAVA_HOME: /usr/lib/jvm/java-1.8.0
      BootstrapActions:
        - Name: DownloadScripts
          ScriptBootstrapAction:
            Args:
              - !Join ['', ['s3://', !Ref pS3EMRBucketName]]
              - us-east-1
              - !Join ['/', ['s3:/', !Ref pS3EMRBucketName, !Ref pExperianLANid, !Ref pS3StepsPath]]
            Path: !Join ['', ['s3://', !Ref pS3EMRBucketName, '/', !Ref pS3BAPath]]
      Instances:
        AdditionalMasterSecurityGroups:
          - !ImportValue SquidProxyClientSG
          - !If [DoNoTerminate, !Ref 'pEMRSSHSg', !Ref 'AWS::NoValue']
        AdditionalSlaveSecurityGroups:
          - !ImportValue SquidProxyClientSG
          - !If [DoNoTerminate, !Ref 'pEMRSSHSg', !Ref 'AWS::NoValue']
        CoreInstanceGroup:
          EbsConfiguration:
            EbsBlockDeviceConfigs:
            - VolumeSpecification:
                SizeInGB: !Ref 'pCoreStorageVolume'
                VolumeType: gp2
              VolumesPerInstance: '1'
            EbsOptimized: 'true'
          InstanceCount: !Ref 'pCoreInstanceCount'
          InstanceType: !Ref 'pCoreInstanceType'
          Market: !If [UseSpot, SPOT, ON_DEMAND]
          BidPrice: !If [UseSpot, !Ref pCoreBidPrice, !Ref 'AWS::NoValue']
          Name: Core Instance
        Ec2KeyName: !If [DoNoTerminate, !Ref 'pKeyName', !Ref 'AWS::NoValue']
        Ec2SubnetId: !ImportValue R8TransientSubnet
        MasterInstanceGroup:
          InstanceCount: '1'
          InstanceType: !Ref 'pMasterInstanceType'
          Market: ON_DEMAND
          Name: Master Instance
        TerminationProtected: 'false'
      JobFlowRole: EMR_EC2_DefaultRole
      LogUri: !Join ['/', ['s3:/', !Ref 'pS3EMRBucketName', !Ref 'pExperianLANid', !Ref 'pEMRLogDir']]
      Name: !Join ['', [!Ref 'pEMRClusterName', '-', !Ref 'pExperianLANid', !If [DoNoTerminate, '-no-terminate', '-auto-terminate']]]
      ReleaseLabel: !Ref 'pEmrReleaseLabel'
      ServiceRole: EMR_DefaultRole
      Tags:
      - Key: Name
        Value: EMR Sample Cluster
      VisibleToAllUsers: 'true'
    Type: AWS::EMR::Cluster
#  SparkStep:
#    Properties:
#      ActionOnFailure: CONTINUE
#      HadoopJarStep:
#        Args:
#        - spark-submit
#        - --deploy-mode
#        - cluster
#        - --class
#        - org.apache.spark.examples.SparkPi
#        - /usr/lib/spark/examples/jars/spark-examples.jar
#        - '10'
#        Jar: command-runner.jar
#        MainClass: ''
#      JobFlowId: !Ref 'rEMRCluster'
#      Name: SparkStep
#    Type: AWS::EMR::Step
#  EMRSampleScriptRunner:
#    Properties:
#      ActionOnFailure: CONTINUE
#      HadoopJarStep:
#        Args:
#        - s3://r8-aws-emr/admin/steps/emr-scipt-runner.sh
#        Jar: s3://r8-aws-emr/admin/steps/script-runner.jar
#        MainClass: ''
#      JobFlowId: !Ref 'rEMRCluster'
#      Name: EMRSampleScriptRunner
#    Type: AWS::EMR::Step
#  DoNoTerminate:
#    Properties:
#      ActionOnFailure: CONTINUE
#      HadoopJarStep:
#        Args:
#        - aws
#        - emr
#        - terminate-clusters
#        - --cluster-ids
#        - !Ref rEMRCluster
#        Jar: command-runner.jar
#        MainClass: ''
#      JobFlowId: !Ref 'rEMRCluster'
#      Name: DoNoTerminate
#    Type: AWS::EMR::Step
  RunSteps:
    Condition: Terminate
    Properties:
      ActionOnFailure: CONTINUE
      HadoopJarStep:
        Args:
        - /tmp/emrsteps/update-emr-steps.sh
        - !Ref rEMRCluster
        - !Ref 'AWS::StackName'
        - !ImportValue SquidProxyDNS
        - !ImportValue SquidProxyPort
        - us-east-1
        Jar: command-runner.jar
        MainClass: ''
      JobFlowId: !Ref 'rEMRCluster'
      Name: RunSteps
    Type: AWS::EMR::Step